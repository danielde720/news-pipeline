{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\nThis notebook is used for reading JSON data from an S3 bucket, flattening the nested schema, and saving it as a Parquet file.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "####  Enviroment Setup\n\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n%connections \"news-connect\"\n\n",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.2 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nConnections to be included:\nnews-connect\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Libaries ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\nfrom awsglue.dynamicframe import DynamicFrame\nfrom pyspark.sql.types import StructType, StructField, StringType, DateType,IntegerType, LongType\nfrom pyspark.sql.functions import col, monotonically_increasing_id, row_number\nfrom pyspark.sql.window import Window\n\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 5537d7f8-facc-41ea-8aff-b6c1adfb816a\nApplying the following default arguments:\n--glue_kernel_version 1.0.2\n--enable-glue-datacatalog true\nWaiting for session 5537d7f8-facc-41ea-8aff-b6c1adfb816a to get into ready status...\nSession 5537d7f8-facc-41ea-8aff-b6c1adfb816a has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Initiazling context\n",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "sc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Fetching current Date ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "current_date = datetime.now().strftime(\"%Y-%m-%d\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Data Loading\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "input_path = f\"s3://news-etl-09-08-23/raw-data/{current_date}/all_news_{current_date}.json\"\ndf = spark.read.option(\"multiline\", \"true\").option(\"inferschema\", \"true\").json(input_path)\ndf.printSchema()                                                                              \n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- author: string (nullable = true)\n |-- content: string (nullable = true)\n |-- description: string (nullable = true)\n |-- publishedAt: string (nullable = true)\n |-- source: struct (nullable = true)\n |    |-- id: string (nullable = true)\n |    |-- name: string (nullable = true)\n |-- title: string (nullable = true)\n |-- url: string (nullable = true)\n |-- urlToImage: string (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Filtering",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df_filtered = df.filter(df.author.isNotNull())",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Flattening Data",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df_filtered = df_filtered.withColumn(\"source_name\", col(\"source.name\")) \\\n                         .withColumn(\"source_id\", col(\"source.id\")) \\\n                         .drop(\"source\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Validating The Filter",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Count any remaining records with a null author\nnull_author_count = df_filtered.filter(df_filtered.author.isNull()).count()\n\n# Print the count or raise an error if any null authors are found\nif null_author_count > 0:\n    raise ValueError(f\"Null authors found in the data: {null_author_count}\")\nelse:\n    print(\"No null authors found in the data.\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "No null authors found in the data.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Creating distinct tables for authors and sources with unique IDs",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "author_table = df_filtered.select(\"author\").distinct().withColumn(\"author_id\", monotonically_increasing_id().cast(LongType()))\nsource_table = df_filtered.select(\"source_name\").distinct().withColumn(\"source_id\", monotonically_increasing_id().cast(LongType()))\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Renaming the key columns to avoid ambiguity",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "author_table = author_table.withColumnRenamed(\"author\", \"unique_author\")\nsource_table = source_table.withColumnRenamed(\"source_name\", \"unique_source_name\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Joining Back",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Removing the original 'source_id' and 'author_id' if they exist in df_filtered\nif 'source_id' in df_filtered.columns:\n    df_filtered = df_filtered.drop('source_id')\nif 'author_id' in df_filtered.columns:\n    df_filtered = df_filtered.drop('author_id')\n\n# Performing the joins with author_table and source_table\ndf_joined = df_filtered.join(author_table, df_filtered[\"author\"] == author_table[\"unique_author\"], \"left_outer\") \\\n                       .join(source_table, df_filtered[\"source_name\"] == source_table[\"unique_source_name\"], \"left_outer\") \\\n                       .drop(\"unique_author\") \\\n                       .drop(\"unique_source_name\")\n\n# Creating the Articles Table with associated author_id and source_id from joined table\narticles_table = df_joined.select(\n    monotonically_increasing_id().alias(\"article_id\"), \n    \"title\", \n    \"description\", \n    \"url\", \n    \"urlToImage\", \n    \"publishedAt\", \n    \"content\", \n    \"author_id\", \n    \"source_id\"\n)\n\n# Show the table for verification\narticles_table.show(5)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------------+\n|article_id|               title|         description|                 url|          urlToImage|         publishedAt|             content|   author_id|   source_id|\n+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------------+\n|         0|Jake Paul Says Hi...|American YouTuber...|https://bleacherr...|https://media.ble...|2023-11-15T22:51:55Z|Sam Hodde/Getty I...|  8589934592|103079215104|\n|         1|How to watch Tyso...|The top heavyweig...|https://www.digit...|https://www.digit...|2023-10-28T16:00:55Z|ESPN\nThe lineal ...|  8589934593| 68719476736|\n|         2|Canelo Alvarez vs...|Saul Alvarez&apos...|https://www.marca...|https://phantom-m...|2023-10-21T04:40:04Z|Saul Alvarez's la...| 51539607552| 85899345920|\n|         3|Joshua, Wilder an...|The boxing world ...|https://www.espn....|https://a3.espncd...|2023-11-17T13:21:53Z|Boxing fans will ...| 25769803776| 42949672960|\n|         4|El triángulo amor...|La relación de Sa...|https://www.mundo...|https://www.mundo...|2023-11-01T10:15:32Z|La relación de Sa...|120259084288| 42949672961|\n+----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Writing To Redshift",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "# Define Redshift connection options\nredshift_connection_options = {\n    \"url\": \"jdbc:redshift://default-workgroup.505802839350.us-west-1.redshift-serverless.amazonaws.com:5439/dev\",\n    \"database\": \"dev\", \n    \"user\": \"admin\",  \n    \"password\": \"admin\",  \n    \"aws_iam_role\": \"arn:aws:iam::505802839350:role/service-role/AWSGlueServiceRole-news\"\n}\n\n# Define Redshift connection\ndef write_to_redshift(dataframe, table_name):\n    connection_options = redshift_connection_options.copy()\n    connection_options[\"dbtable\"] = table_name\n    dynamic_frame = DynamicFrame.fromDF(dataframe, glueContext, table_name)\n    glueContext.write_dynamic_frame.from_jdbc_conf(\n        frame=dynamic_frame,\n        catalog_connection=\"news-connect\",\n        connection_options=connection_options,\n        redshift_tmp_dir=\"s3://redshift-temp-buck/glue-temp/\"  # Hardcoded TempDir\n    )\n\n# Write the author table to Redshift\nwrite_to_redshift(author_table, \"public.author\")\n\n# Write the source table to Redshift\nwrite_to_redshift(source_table, \"public.source\")\n\n# Write the article table to Redshift\nwrite_to_redshift(article_table, \"public.article\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}