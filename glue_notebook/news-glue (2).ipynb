{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\nThis notebook is used for reading JSON data from an S3 bucket, flattening the nested schema, and saving it as a Parquet file.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "####  Enviroment Setup\n\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%idle_timeout 2880\n%glue_version 3.0\n%worker_type G.1X\n%number_of_workers 5\n\n",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 0.38.1 \nCurrent idle_timeout is 2800 minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 3.0\nPrevious worker type: G.1X\nSetting new worker type to: G.1X\nPrevious number of workers: 5\nSetting new number of workers to: 5\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "### Libaries ",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql import functions as F\nfrom datetime import datetime\n\n\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Authenticating with environment variables and user-defined glue_role_arn: arn:aws:iam::505802839350:role/service-role/AWSGlueServiceRole-news\nTrying to create a Glue session for the kernel.\nWorker Type: G.1X\nNumber of Workers: 5\nSession ID: 0e65cfee-a77d-4928-bc53-cecb47f390f3\nJob Type: glueetl\nApplying the following default arguments:\n--glue_kernel_version 0.38.1\n--enable-glue-datacatalog true\nWaiting for session 0e65cfee-a77d-4928-bc53-cecb47f390f3 to get into ready status...\nSession 0e65cfee-a77d-4928-bc53-cecb47f390f3 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Initiazling context\n",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "sc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Data Loading\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "input_path = f\"s3://news-etl-09-08-23/raw-data/{current_date}/all_news_{current_date}.json\"\ndf = spark.read.option(\"multiline\", \"true\").option(\"inferschema\", \"true\").json(input_path)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Data Transformation\n",
			"metadata": {
				"tags": []
			}
		},
		{
			"cell_type": "code",
			"source": "df_flattened = df.select(\n    \"author\",\n    \"content\",\n    \"description\",\n    \"publishedAt\",\n    F.col(\"source.id\").alias(\"source_id\"),\n    F.col(\"source.name\").alias(\"source_name\"),\n    \"title\",\n    \"url\",\n    \"urlToImage\"\n)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 10,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Data Saving\n",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "output_path = f\"s3://news-etl-09-08-23/transformed-data/all_news_flattened_{current_date}.parquet\"\ndf_flattened.write.parquet(output_path)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Validation",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "df_parquet = spark.read.parquet(output_path)\ndf_parquet.printSchema()\ndf_parquet.show(5)\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 12,
			"outputs": [
				{
					"name": "stdout",
					"text": "root\n |-- author: string (nullable = true)\n |-- content: string (nullable = true)\n |-- description: string (nullable = true)\n |-- publishedAt: string (nullable = true)\n |-- source_id: string (nullable = true)\n |-- source_name: string (nullable = true)\n |-- title: string (nullable = true)\n |-- url: string (nullable = true)\n |-- urlToImage: string (nullable = true)\n\n+--------------------+--------------------+--------------------+--------------------+----------------+----------------+--------------------+--------------------+--------------------+\n|              author|             content|         description|         publishedAt|       source_id|     source_name|               title|                 url|          urlToImage|\n+--------------------+--------------------+--------------------+--------------------+----------------+----------------+--------------------+--------------------+--------------------+\n|    Quentyn Kennemer|Watch Canelo defe...|Canelo Alvarez pu...|2023-09-30T22:00:01Z|business-insider|Business Insider|Where to watch Ca...|https://www.busin...|https://i.insider...|\n|      Mike Coppinger|Canelo Alvarez an...|Canelo Alvarez an...|2023-10-01T00:22:37Z|            espn|            ESPN|Canelo Alvarez-Je...|https://www.espn....|https://a1.espncd...|\n|Damian L. Delgado...|Sep 27, 2023, 07:...|Jermell Charlo is...|2023-09-27T12:08:43Z|            espn|            ESPN|Moving on up: Is ...|https://www.espn....|https://a4.espncd...|\n|                ESPN|Sep 21, 2023, 07:...|From surprising o...|2023-09-21T12:20:08Z|            espn|            ESPN|Bold predictions ...|https://www.espn....|https://a.espncdn...|\n|       Oscar Hartzog|If you purchase a...|For the first tim...|2023-09-30T17:00:00Z|            null|   Rolling Stone|Canelo vs. Charlo...|https://www.rolli...|https://www.rolli...|\n+--------------------+--------------------+--------------------+--------------------+----------------+----------------+--------------------+--------------------+--------------------+\nonly showing top 5 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "markdown",
			"source": "#### Conclusion",
			"metadata": {}
		},
		{
			"cell_type": "code",
			"source": "Successfully flattened the nested JSON schema and saved the data as a Parquet file in S3.\n",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}